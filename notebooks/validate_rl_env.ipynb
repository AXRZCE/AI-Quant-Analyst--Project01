{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate RL Environment\n",
    "\n",
    "This notebook validates the reinforcement learning environment for trading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add src directory to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Set up plotting\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import RL Environment\n",
    "\n",
    "First, let's import the RL environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import RL environment\n",
    "from rl.env import TradingEnv, MultiAssetTradingEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data\n",
    "\n",
    "Let's load the data for the RL environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Try to load data from different sources\n",
    "try:\n",
    "    # Try to load from batch features\n",
    "    df = pd.read_parquet(\"../data/features/batch/technical/*.parquet\")\n",
    "    print(\"Loaded data from batch features.\")\n",
    "except Exception:\n",
    "    try:\n",
    "        # Try to load from processed data\n",
    "        df = pd.read_parquet(\"../data/processed/training_data.parquet\")\n",
    "        print(\"Loaded data from processed data.\")\n",
    "    except Exception:\n",
    "        try:\n",
    "            # Try to load from raw data\n",
    "            df = pd.read_parquet(\"../data/raw/ticks/*/*.parquet\")\n",
    "            print(\"Loaded data from raw data.\")\n",
    "        except Exception:\n",
    "            # Create dummy data\n",
    "            print(\"No data found. Creating dummy data.\")\n",
    "            \n",
    "            # Create dummy data\n",
    "            np.random.seed(42)\n",
    "            n_samples = 100\n",
    "            \n",
    "            # Create price data\n",
    "            symbols = [\"AAPL\", \"MSFT\", \"GOOGL\"]\n",
    "            data = []\n",
    "            \n",
    "            for symbol in symbols:\n",
    "                # Generate random walk\n",
    "                if symbol == \"AAPL\":\n",
    "                    price = 150.0\n",
    "                elif symbol == \"MSFT\":\n",
    "                    price = 250.0\n",
    "                else:  # GOOGL\n",
    "                    price = 2000.0\n",
    "                \n",
    "                prices = [price]\n",
    "                for _ in range(n_samples - 1):\n",
    "                    # Random price change\n",
    "                    price_change = np.random.normal(0, 1) * price * 0.01\n",
    "                    price += price_change\n",
    "                    prices.append(price)\n",
    "                \n",
    "                # Create DataFrame\n",
    "                for i, price in enumerate(prices):\n",
    "                    data.append({\n",
    "                        \"symbol\": symbol,\n",
    "                        \"timestamp\": pd.Timestamp(\"2023-01-01\") + pd.Timedelta(days=i),\n",
    "                        \"close\": price,\n",
    "                        \"open\": price * (1 - np.random.random() * 0.01),\n",
    "                        \"high\": price * (1 + np.random.random() * 0.01),\n",
    "                        \"low\": price * (1 - np.random.random() * 0.01),\n",
    "                        \"volume\": np.random.randint(1000, 10000),\n",
    "                        \"ma_5\": np.random.random() * 0.1,\n",
    "                        \"rsi_14\": np.random.random() * 100\n",
    "                    })\n",
    "            \n",
    "            # Create DataFrame\n",
    "            df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Display the first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check data types\n",
    "print(\"Data types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Check unique symbols\n",
    "print(\"\\nUnique symbols:\")\n",
    "print(df[\"symbol\"].unique())\n",
    "\n",
    "# Check date range\n",
    "if \"timestamp\" in df.columns:\n",
    "    print(\"\\nDate range:\")\n",
    "    print(f\"Start: {df['timestamp'].min()}\")\n",
    "    print(f\"End: {df['timestamp'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize RL Environment\n",
    "\n",
    "Now, let's initialize the RL environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define feature columns\n",
    "feature_cols = [\"ma_5\", \"rsi_14\"]\n",
    "\n",
    "# Check if feature columns exist in the DataFrame\n",
    "for col in feature_cols:\n",
    "    if col not in df.columns:\n",
    "        print(f\"Feature column '{col}' not found in DataFrame. Available columns: {df.columns.tolist()}\")\n",
    "        # Try to find alternative feature columns\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        # Exclude price and volume columns\n",
    "        exclude_cols = [\"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
    "        feature_cols = [col for col in numeric_cols if col not in exclude_cols]\n",
    "        print(f\"Using alternative feature columns: {feature_cols}\")\n",
    "        break\n",
    "\n",
    "# Initialize environment\n",
    "env = TradingEnv(\n",
    "    df=df,\n",
    "    feature_cols=feature_cols,\n",
    "    initial_capital=100_000,\n",
    "    transaction_cost=0.001\n",
    ")\n",
    "\n",
    "# Print environment information\n",
    "print(\"Environment initialized.\")\n",
    "print(f\"Observation space: {env.observation_space}\")\n",
    "print(f\"Action space: {env.action_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Environment Reset\n",
    "\n",
    "Let's test the environment reset function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Reset environment\n",
    "obs = env.reset()\n",
    "\n",
    "# Print observation\n",
    "print(f\"Observation shape: {obs.shape}\")\n",
    "print(f\"Observation: {obs}\")\n",
    "\n",
    "# Print environment state\n",
    "print(f\"Current step: {env.current_step}\")\n",
    "print(f\"Cash: ${env.cash:.2f}\")\n",
    "print(f\"Positions: {env.positions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Environment Step\n",
    "\n",
    "Now, let's test the environment step function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Take a step with a random action\n",
    "action = env.action_space.sample()\n",
    "obs, reward, done, info = env.step(action)\n",
    "\n",
    "# Print step results\n",
    "print(f\"Action: {action}\")\n",
    "print(f\"Reward: {reward:.6f}\")\n",
    "print(f\"Done: {done}\")\n",
    "print(f\"Info: {info}\")\n",
    "\n",
    "# Print environment state\n",
    "print(f\"Current step: {env.current_step}\")\n",
    "print(f\"Cash: ${env.cash:.2f}\")\n",
    "print(f\"Positions: {env.positions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run a Complete Episode\n",
    "\n",
    "Let's run a complete episode with random actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Reset environment\n",
    "obs = env.reset()\n",
    "\n",
    "# Run episode\n",
    "done = False\n",
    "total_reward = 0\n",
    "step_count = 0\n",
    "\n",
    "while not done:\n",
    "    # Take a step with a random action\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    \n",
    "    # Update total reward\n",
    "    total_reward += reward\n",
    "    step_count += 1\n",
    "    \n",
    "    # Render environment\n",
    "    env.render()\n",
    "    \n",
    "    # Break if too many steps\n",
    "    if step_count >= 100:\n",
    "        print(\"Maximum steps reached.\")\n",
    "        break\n",
    "\n",
    "# Print episode results\n",
    "print(f\"Episode completed after {step_count} steps.\")\n",
    "print(f\"Total reward: {total_reward:.2f}\")\n",
    "print(f\"Final portfolio value: ${info['portfolio_value']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analyze Portfolio History\n",
    "\n",
    "Let's analyze the portfolio history from the episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Get portfolio history\n",
    "history_df = env.get_portfolio_history()\n",
    "\n",
    "# Display portfolio history\n",
    "history_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot portfolio history\n",
    "fig = env.plot_portfolio_history()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate portfolio statistics\n",
    "initial_value = history_df[\"portfolio_value\"].iloc[0]\n",
    "final_value = history_df[\"portfolio_value\"].iloc[-1]\n",
    "total_return = (final_value / initial_value - 1) * 100\n",
    "\n",
    "# Calculate daily returns\n",
    "history_df[\"daily_return\"] = history_df[\"portfolio_value\"].pct_change()\n",
    "\n",
    "# Calculate annualized return and volatility\n",
    "if len(history_df) > 1:\n",
    "    annualized_return = ((final_value / initial_value) ** (252 / len(history_df)) - 1) * 100\n",
    "    annualized_volatility = history_df[\"daily_return\"].std() * np.sqrt(252) * 100\n",
    "    sharpe_ratio = annualized_return / annualized_volatility if annualized_volatility > 0 else 0\n",
    "else:\n",
    "    annualized_return = 0\n",
    "    annualized_volatility = 0\n",
    "    sharpe_ratio = 0\n",
    "\n",
    "# Print portfolio statistics\n",
    "print(\"Portfolio Statistics:\")\n",
    "print(f\"Initial Value: ${initial_value:.2f}\")\n",
    "print(f\"Final Value: ${final_value:.2f}\")\n",
    "print(f\"Total Return: {total_return:.2f}%\")\n",
    "print(f\"Annualized Return: {annualized_return:.2f}%\")\n",
    "print(f\"Annualized Volatility: {annualized_volatility:.2f}%\")\n",
    "print(f\"Sharpe Ratio: {sharpe_ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test Multi-Asset Environment\n",
    "\n",
    "Now, let's test the multi-asset environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize multi-asset environment\n",
    "multi_env = MultiAssetTradingEnv(\n",
    "    df=df,\n",
    "    feature_cols=feature_cols,\n",
    "    initial_capital=100_000,\n",
    "    transaction_cost=0.001\n",
    ")\n",
    "\n",
    "# Print environment information\n",
    "print(\"Multi-asset environment initialized.\")\n",
    "print(f\"Observation space: {multi_env.observation_space}\")\n",
    "print(f\"Action space: {multi_env.action_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Reset environment\n",
    "obs = multi_env.reset()\n",
    "\n",
    "# Run episode\n",
    "done = False\n",
    "total_reward = 0\n",
    "step_count = 0\n",
    "\n",
    "while not done:\n",
    "    # Take a step with a random action\n",
    "    action = multi_env.action_space.sample()\n",
    "    obs, reward, done, info = multi_env.step(action)\n",
    "    \n",
    "    # Update total reward\n",
    "    total_reward += reward\n",
    "    step_count += 1\n",
    "    \n",
    "    # Render environment\n",
    "    multi_env.render()\n",
    "    \n",
    "    # Break if too many steps\n",
    "    if step_count >= 10:\n",
    "        print(\"Maximum steps reached.\")\n",
    "        break\n",
    "\n",
    "# Print episode results\n",
    "print(f\"Episode completed after {step_count} steps.\")\n",
    "print(f\"Total reward: {total_reward:.2f}\")\n",
    "print(f\"Final portfolio value: ${info['portfolio_value']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Get portfolio history\n",
    "multi_history_df = multi_env.get_portfolio_history()\n",
    "\n",
    "# Plot portfolio history\n",
    "fig = multi_env.plot_portfolio_history()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Implement a Simple Trading Strategy\n",
    "\n",
    "Let's implement a simple trading strategy and test it in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define a simple trading strategy\n",
    "def simple_strategy(env, obs):\n",
    "    \"\"\"\n",
    "    A simple trading strategy based on RSI.\n",
    "    \n",
    "    Args:\n",
    "        env: Trading environment\n",
    "        obs: Current observation\n",
    "        \n",
    "    Returns:\n",
    "        Action to take\n",
    "    \"\"\"\n",
    "    # Get RSI values for each symbol\n",
    "    rsi_values = {}\n",
    "    for i, symbol in enumerate(env.symbols):\n",
    "        # Find RSI column index\n",
    "        rsi_idx = None\n",
    "        for j, col in enumerate(env.feature_cols):\n",
    "            if \"rsi\" in col.lower():\n",
    "                rsi_idx = j + i * len(env.feature_cols)\n",
    "                break\n",
    "        \n",
    "        if rsi_idx is not None:\n",
    "            rsi_values[symbol] = obs[rsi_idx]\n",
    "        else:\n",
    "            # If RSI not found, use a random value\n",
    "            rsi_values[symbol] = np.random.random() * 100\n",
    "    \n",
    "    # Determine action for each symbol\n",
    "    symbol_actions = {}\n",
    "    for symbol, rsi in rsi_values.items():\n",
    "        if rsi < 30:  # Oversold, buy\n",
    "            symbol_actions[symbol] = 1\n",
    "        elif rsi > 70:  # Overbought, sell\n",
    "            symbol_actions[symbol] = 2\n",
    "        else:  # Neutral, hold\n",
    "            symbol_actions[symbol] = 0\n",
    "    \n",
    "    # Encode action\n",
    "    action = 0\n",
    "    for i, symbol in enumerate(env.symbols):\n",
    "        action += symbol_actions[symbol] * (3 ** i)\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Reset environment\n",
    "obs = env.reset()\n",
    "\n",
    "# Run episode with simple strategy\n",
    "done = False\n",
    "total_reward = 0\n",
    "step_count = 0\n",
    "\n",
    "while not done:\n",
    "    # Take a step with the simple strategy\n",
    "    action = simple_strategy(env, obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    \n",
    "    # Update total reward\n",
    "    total_reward += reward\n",
    "    step_count += 1\n",
    "    \n",
    "    # Render environment\n",
    "    env.render()\n",
    "    \n",
    "    # Break if too many steps\n",
    "    if step_count >= 100:\n",
    "        print(\"Maximum steps reached.\")\n",
    "        break\n",
    "\n",
    "# Print episode results\n",
    "print(f\"Episode completed after {step_count} steps.\")\n",
    "print(f\"Total reward: {total_reward:.2f}\")\n",
    "print(f\"Final portfolio value: ${info['portfolio_value']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Get portfolio history\n",
    "strategy_history_df = env.get_portfolio_history()\n",
    "\n",
    "# Plot portfolio history\n",
    "fig = env.plot_portfolio_history()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compare random strategy with simple strategy\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.plot(history_df[\"step\"], history_df[\"portfolio_value\"], label=\"Random Strategy\")\n",
    "plt.plot(strategy_history_df[\"step\"], strategy_history_df[\"portfolio_value\"], label=\"Simple Strategy\")\n",
    "\n",
    "plt.title(\"Portfolio Value Comparison\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Portfolio Value ($)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "In this notebook, we have validated the reinforcement learning environment for trading. We have:\n",
    "\n",
    "1. Initialized the RL environment\n",
    "2. Tested the environment reset and step functions\n",
    "3. Run a complete episode with random actions\n",
    "4. Analyzed the portfolio history\n",
    "5. Tested the multi-asset environment\n",
    "6. Implemented a simple trading strategy\n",
    "\n",
    "The RL environment provides a flexible framework for developing and testing trading strategies using reinforcement learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
