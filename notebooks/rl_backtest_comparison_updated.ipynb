{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL vs. Baseline Backtest Comparison\n",
    "\n",
    "This notebook loads price & feature data, applies both the RL policy and the XGBoost baseline, runs them through the backtester, and compares final PnL and trade logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 1. Imports & Setup\n",
    "import pandas as pd\n",
    "from joblib import load\n",
    "import matplotlib.pyplot as plt\n",
    "from src.backtest.backtester import Backtester\n",
    "from src.rl.evaluate_rl import evaluate  # returns total_reward & (optionally) per-step preds\n",
    "\n",
    "# Adjust these paths as needed\n",
    "DATA_PATH = \"data/features/batch/technical_with_timeidx.parquet\"\n",
    "RL_CHECKPOINT = \"logs/rl/PPO_Trading/checkpoint_000050/checkpoint-50\"\n",
    "FEATURE_COLS = [\"ma_5\", \"rsi_14\", \"close\"]\n",
    "BASELINE_MODEL_PATH = \"models/baseline_xgb.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data & Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 2.1 Load your historical feature+price data\n",
    "df = pd.read_parquet(DATA_PATH)\n",
    "\n",
    "# 2.2 Ensure sorted by timestamp\n",
    "df = df.sort_values([\"symbol\",\"time_idx\"]).reset_index(drop=True)\n",
    "\n",
    "# 2.3 Display a sample\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Predictions\n",
    "\n",
    "- **RL Policy**: we'll step through the environment and record `pred_rl`  \n",
    "- **Baseline**: direct model.predict on feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 3.1 Baseline predictions\n",
    "baseline = load(BASELINE_MODEL_PATH)\n",
    "X = df[FEATURE_COLS]\n",
    "df[\"pred_baseline\"] = baseline.predict(X)\n",
    "\n",
    "# 3.2 RL predictions: run env once to collect per-step actions as predictions\n",
    "#    We'll monkeyâ€‘patch evaluate to return an array of actions or rewards; \n",
    "#    here, assume evaluate_rl.modify it to also return per-step preds as `pred_rl`.\n",
    "#    If not, you can manually step the env:\n",
    "\n",
    "from src.rl.env import TradingEnv\n",
    "env = TradingEnv(df, feature_cols=FEATURE_COLS)\n",
    "obs = env.reset()\n",
    "preds = []\n",
    "done = False\n",
    "\n",
    "# Check if RL checkpoint exists\n",
    "import os\n",
    "if not os.path.exists(RL_CHECKPOINT):\n",
    "    print(f\"RL checkpoint not found: {RL_CHECKPOINT}\")\n",
    "    print(\"Using random actions instead\")\n",
    "    # Use random actions\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        # translate action into price-movement prediction: \n",
    "        #   e.g. pred_rl = +1 if buy, -1 if sell, 0 for hold\n",
    "        preds.append(1 if action==1 else (-1 if action==2 else 0))\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "else:\n",
    "    # Use RL agent\n",
    "    import ray\n",
    "    from ray.rllib.agents.ppo import PPOTrainer\n",
    "    from ray.tune.registry import register_env\n",
    "    \n",
    "    # Define environment creator\n",
    "    def env_creator(cfg):\n",
    "        df = pd.read_parquet(cfg[\"data_path\"])\n",
    "        return TradingEnv(df, feature_cols=cfg[\"feature_cols\"])\n",
    "    \n",
    "    # Initialize Ray\n",
    "    ray.init(ignore_reinit_error=True)\n",
    "    \n",
    "    # Register environment\n",
    "    register_env(\"trading_env\", lambda cfg: env_creator(cfg))\n",
    "    \n",
    "    # Load agent\n",
    "    trainer = PPOTrainer(env=\"trading_env\", config={\n",
    "        \"env_config\":{\"data_path\": DATA_PATH, \"feature_cols\": FEATURE_COLS},\n",
    "        \"framework\":\"torch\"\n",
    "    })\n",
    "    trainer.restore(RL_CHECKPOINT)\n",
    "    \n",
    "    # Run agent\n",
    "    while not done:\n",
    "        action = trainer.compute_action(obs)\n",
    "        # translate action into price-movement prediction: \n",
    "        #   e.g. pred_rl = +1 if buy, -1 if sell, 0 for hold\n",
    "        preds.append(1 if action==1 else (-1 if action==2 else 0))\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "\n",
    "df = df.iloc[:len(preds)].copy()\n",
    "df[\"pred_rl\"] = preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Backtest Both Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 4.1 Backtest baseline\n",
    "bt_base = Backtester(df.assign(pred=df[\"pred_baseline\"]), None)\n",
    "results_base = bt_base.run()\n",
    "if len(results_base) == 3:\n",
    "    final_base, trades_base, portfolio_base = results_base\n",
    "else:\n",
    "    final_base, trades_base = results_base\n",
    "    portfolio_base = pd.DataFrame({\"timestamp\": df[\"timestamp\"].unique(), \"portfolio_value\": [final_base] * len(df[\"timestamp\"].unique())})\n",
    "print(f\"Baseline final capital: {final_base:.2f}\")\n",
    "display(trades_base.head())\n",
    "\n",
    "# 4.2 Backtest RL\n",
    "bt_rl = Backtester(df.assign(pred=df[\"pred_rl\"]), None)\n",
    "results_rl = bt_rl.run()\n",
    "if len(results_rl) == 3:\n",
    "    final_rl, trades_rl, portfolio_rl = results_rl\n",
    "else:\n",
    "    final_rl, trades_rl = results_rl\n",
    "    portfolio_rl = pd.DataFrame({\"timestamp\": df[\"timestamp\"].unique(), \"portfolio_value\": [final_rl] * len(df[\"timestamp\"].unique())})\n",
    "print(f\"RL final capital: {final_rl:.2f}\")\n",
    "display(trades_rl.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visual Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 5.1 Equity Curves\n",
    "# Simulate equity over time for each strategy\n",
    "def equity_curve(df, trades):\n",
    "    curve = []\n",
    "    cash = 100_000\n",
    "    pos = 0\n",
    "    trade_idx = 0\n",
    "    for idx, row in df.iterrows():\n",
    "        # apply any trade at this timestamp\n",
    "        if trade_idx < len(trades) and trades.iloc[trade_idx][\"timestamp\"] == row[\"timestamp\"]:\n",
    "            action = trades.iloc[trade_idx][\"action\"]\n",
    "            price  = trades.iloc[trade_idx][\"price\"]\n",
    "            if action==\"BUY\":  pos = cash / price; cash = 0\n",
    "            if action in (\"SELL\",\"FINAL_CLOSE\"): cash = pos * price; pos = 0\n",
    "            trade_idx += 1\n",
    "        curve.append(cash + pos * row[\"close\"])\n",
    "    return pd.Series(curve, index=df[\"timestamp\"])\n",
    "\n",
    "# Use portfolio history directly if available\n",
    "if \"portfolio_value\" in portfolio_base.columns and \"portfolio_value\" in portfolio_rl.columns:\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot(portfolio_base[\"timestamp\"], portfolio_base[\"portfolio_value\"], label=\"Baseline\")\n",
    "    plt.plot(portfolio_rl[\"timestamp\"], portfolio_rl[\"portfolio_value\"], label=\"RL\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Equity Curve Comparison\")\n",
    "    plt.ylabel(\"Portfolio Value\")\n",
    "    plt.grid(True)\n",
    "else:\n",
    "    # Calculate equity curves\n",
    "    eq_base = equity_curve(df, trades_base)\n",
    "    eq_rl   = equity_curve(df, trades_rl)\n",
    "\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot(eq_base, label=\"Baseline\")\n",
    "    plt.plot(eq_rl,   label=\"RL\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Equity Curve Comparison\")\n",
    "    plt.ylabel(\"Portfolio Value\")\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary & Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate performance metrics\n",
    "def calculate_metrics(portfolio_df, trades_df):\n",
    "    # Calculate daily returns\n",
    "    portfolio_df[\"daily_return\"] = portfolio_df[\"portfolio_value\"].pct_change()\n",
    "    \n",
    "    # Calculate total return\n",
    "    initial_value = portfolio_df[\"portfolio_value\"].iloc[0]\n",
    "    final_value = portfolio_df[\"portfolio_value\"].iloc[-1]\n",
    "    total_return = (final_value / initial_value - 1) * 100\n",
    "    \n",
    "    # Calculate Sharpe ratio\n",
    "    risk_free_rate = 0.02  # 2% annual risk-free rate\n",
    "    daily_risk_free = (1 + risk_free_rate) ** (1 / 252) - 1\n",
    "    excess_returns = portfolio_df[\"daily_return\"] - daily_risk_free\n",
    "    sharpe_ratio = excess_returns.mean() / excess_returns.std() * (252 ** 0.5)  # Annualized\n",
    "    \n",
    "    # Calculate max drawdown\n",
    "    portfolio_df[\"cumulative_return\"] = (1 + portfolio_df[\"daily_return\"]).cumprod()\n",
    "    portfolio_df[\"cumulative_max\"] = portfolio_df[\"cumulative_return\"].cummax()\n",
    "    portfolio_df[\"drawdown\"] = (portfolio_df[\"cumulative_return\"] / portfolio_df[\"cumulative_max\"] - 1) * 100\n",
    "    max_drawdown = portfolio_df[\"drawdown\"].min()\n",
    "    \n",
    "    # Calculate win rate\n",
    "    if len(trades_df) > 0:\n",
    "        winning_trades = trades_df[trades_df[\"pnl\"] > 0]\n",
    "        win_rate = len(winning_trades) / len(trades_df) * 100\n",
    "    else:\n",
    "        win_rate = 0\n",
    "    \n",
    "    return {\n",
    "        \"total_return\": total_return,\n",
    "        \"sharpe_ratio\": sharpe_ratio,\n",
    "        \"max_drawdown\": max_drawdown,\n",
    "        \"win_rate\": win_rate,\n",
    "        \"num_trades\": len(trades_df)\n",
    "    }\n",
    "\n",
    "# Calculate metrics\n",
    "base_metrics = calculate_metrics(portfolio_base, trades_base)\n",
    "rl_metrics = calculate_metrics(portfolio_rl, trades_rl)\n",
    "\n",
    "# Print metrics\n",
    "print(\"Baseline Metrics:\")\n",
    "print(f\"  Final Capital: ${final_base:.2f}\")\n",
    "print(f\"  Total Return: {base_metrics['total_return']:.2f}%\")\n",
    "print(f\"  Sharpe Ratio: {base_metrics['sharpe_ratio']:.2f}\")\n",
    "print(f\"  Max Drawdown: {base_metrics['max_drawdown']:.2f}%\")\n",
    "print(f\"  Win Rate: {base_metrics['win_rate']:.2f}%\")\n",
    "print(f\"  Number of Trades: {base_metrics['num_trades']}\")\n",
    "print()\n",
    "print(\"RL Metrics:\")\n",
    "print(f\"  Final Capital: ${final_rl:.2f}\")\n",
    "print(f\"  Total Return: {rl_metrics['total_return']:.2f}%\")\n",
    "print(f\"  Sharpe Ratio: {rl_metrics['sharpe_ratio']:.2f}\")\n",
    "print(f\"  Max Drawdown: {rl_metrics['max_drawdown']:.2f}%\")\n",
    "print(f\"  Win Rate: {rl_metrics['win_rate']:.2f}%\")\n",
    "print(f\"  Number of Trades: {rl_metrics['num_trades']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary & Insights\n",
    "\n",
    "- **Final Capital**:  \n",
    "  - Baseline: $ {final_base:.2f}  \n",
    "  - RL:       $ {final_rl:.2f}\n",
    "\n",
    "- **Performance Metrics**:\n",
    "  - Baseline: Total Return: {base_metrics['total_return']:.2f}%, Sharpe Ratio: {base_metrics['sharpe_ratio']:.2f}, Win Rate: {base_metrics['win_rate']:.2f}%\n",
    "  - RL: Total Return: {rl_metrics['total_return']:.2f}%, Sharpe Ratio: {rl_metrics['sharpe_ratio']:.2f}, Win Rate: {rl_metrics['win_rate']:.2f}%\n",
    "\n",
    "_Reflect on which strategy performed better and why._\n",
    "\n",
    "The RL strategy has several advantages over the baseline:\n",
    "\n",
    "1. **Adaptability**: The RL agent can adapt to changing market conditions by learning from its interactions with the environment.\n",
    "\n",
    "2. **Sequential Decision Making**: Unlike the baseline model which makes independent predictions, the RL agent considers the entire sequence of decisions and their long-term impact.\n",
    "\n",
    "3. **Risk Management**: The RL agent can learn to manage risk by balancing between aggressive trading for higher returns and conservative approaches to minimize drawdowns.\n",
    "\n",
    "4. **Direct Optimization**: The RL agent directly optimizes for the trading objective (portfolio value) rather than a proxy like price prediction accuracy.\n",
    "\n",
    "Areas for improvement:\n",
    "\n",
    "1. **More Training Data**: The RL agent could benefit from training on more diverse market conditions.\n",
    "\n",
    "2. **Hyperparameter Tuning**: Further tuning of the RL algorithm hyperparameters could improve performance.\n",
    "\n",
    "3. **Feature Engineering**: Adding more features or using different feature representations could help the RL agent make better decisions.\n",
    "\n",
    "4. **Reward Function Design**: Experimenting with different reward functions that better align with trading objectives could enhance performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
