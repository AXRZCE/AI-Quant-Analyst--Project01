{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate Bayesian Model\n",
    "\n",
    "This notebook validates the Bayesian regression model for uncertainty quantification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add src directory to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Set up plotting\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Check if NumPyro is Available\n",
    "\n",
    "First, let's check if NumPyro is available. If not, we'll use a dummy implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Try to import NumPyro\n",
    "try:\n",
    "    import numpyro\n",
    "    import jax.numpy as jnp\n",
    "    import jax.random as random\n",
    "    NUMPYRO_AVAILABLE = True\n",
    "    print(\"NumPyro is available.\")\n",
    "except ImportError:\n",
    "    NUMPYRO_AVAILABLE = False\n",
    "    print(\"NumPyro is not available. Please install it with 'pip install numpyro jax jaxlib'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data\n",
    "\n",
    "Let's load the data for training the Bayesian model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import data preparation module\n",
    "from uncertainty.data_prep import load_df, prepare_features_targets, split_data, normalize_data\n",
    "\n",
    "# Try to load data from different sources\n",
    "try:\n",
    "    # Try to load from batch features\n",
    "    df = load_df(\"../data/features/batch/technical/*.parquet\")\n",
    "    print(\"Loaded data from batch features.\")\n",
    "except ValueError:\n",
    "    try:\n",
    "        # Try to load from processed data\n",
    "        df = load_df(\"../data/processed/training_data.parquet\")\n",
    "        print(\"Loaded data from processed data.\")\n",
    "    except ValueError:\n",
    "        try:\n",
    "            # Try to load from raw data\n",
    "            df = load_df(\"../data/raw/ticks/*/*.parquet\")\n",
    "            print(\"Loaded data from raw data.\")\n",
    "        except ValueError:\n",
    "            # Create dummy data\n",
    "            print(\"No data found. Creating dummy data.\")\n",
    "            np.random.seed(42)\n",
    "            n_samples = 1000\n",
    "            n_features = 5\n",
    "            X = np.random.randn(n_samples, n_features)\n",
    "            y = X[:, 0] * 0.5 + X[:, 1] * 0.3 + X[:, 2] * 0.2 + np.random.randn(n_samples) * 0.1\n",
    "            \n",
    "            # Create DataFrame\n",
    "            df = pd.DataFrame(X, columns=[f\"feature_{i}\" for i in range(n_features)])\n",
    "            df[\"label\"] = y\n",
    "            df[\"symbol\"] = \"DUMMY\"\n",
    "            df[\"timestamp\"] = pd.date_range(start=\"2023-01-01\", periods=n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Display the first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Prepare features and targets\n",
    "X, y = prepare_features_targets(df)\n",
    "\n",
    "# Split data\n",
    "splits = split_data(X, y, train_size=0.7, val_size=0.15, shuffle=True, random_state=42)\n",
    "\n",
    "# Normalize data\n",
    "normalized = normalize_data(splits[\"X_train\"], splits[\"X_val\"], splits[\"X_test\"])\n",
    "\n",
    "# Print shapes\n",
    "print(f\"X_train shape: {normalized['X_train'].shape}\")\n",
    "print(f\"X_val shape: {normalized['X_val'].shape}\")\n",
    "print(f\"X_test shape: {normalized['X_test'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Bayesian Model\n",
    "\n",
    "Now, let's train the Bayesian regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import Bayesian model\n",
    "from uncertainty.bayesian_model import BayesianRegression\n",
    "\n",
    "if NUMPYRO_AVAILABLE:\n",
    "    # Create and fit model\n",
    "    model = BayesianRegression(\n",
    "        num_warmup=200,\n",
    "        num_samples=500,\n",
    "        num_chains=1,\n",
    "        random_seed=42\n",
    "    )\n",
    "    \n",
    "    # Get feature names\n",
    "    feature_cols = [c for c in df.columns if c not in [\"timestamp\", \"symbol\", \"date\", \"label\"]]\n",
    "    \n",
    "    # Store normalization parameters\n",
    "    model.normalization_params = normalized[\"params\"]\n",
    "    \n",
    "    # Fit model\n",
    "    result = model.fit(normalized[\"X_train\"], splits[\"y_train\"], feature_names=feature_cols)\n",
    "    \n",
    "    print(\"Model fitted successfully.\")\n",
    "else:\n",
    "    print(\"Skipping model training because NumPyro is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Examine Posterior Distribution\n",
    "\n",
    "Let's examine the posterior distribution of the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "if NUMPYRO_AVAILABLE:\n",
    "    # Get summary statistics\n",
    "    summary = model.get_summary()\n",
    "    \n",
    "    # Display summary\n",
    "    display(summary)\n",
    "    \n",
    "    # Plot trace\n",
    "    fig = model.plot_trace()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Skipping posterior examination because NumPyro is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Make Predictions with Uncertainty\n",
    "\n",
    "Now, let's make predictions with uncertainty estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "if NUMPYRO_AVAILABLE:\n",
    "    # Make predictions with credible intervals\n",
    "    predictions = model.predict_interval(normalized[\"X_test\"], interval=0.9)\n",
    "    \n",
    "    # Plot predictions vs actual\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot actual vs predicted\n",
    "    plt.scatter(splits[\"y_test\"], predictions[\"mean\"], alpha=0.5)\n",
    "    \n",
    "    # Plot credible intervals for a subset of points\n",
    "    for i in range(min(100, len(predictions[\"mean\"]))):\n",
    "        plt.plot([splits[\"y_test\"][i], splits[\"y_test\"][i]], \n",
    "                 [predictions[\"lower\"][i], predictions[\"upper\"][i]], \n",
    "                 color=\"red\", alpha=0.3)\n",
    "    \n",
    "    # Plot diagonal line\n",
    "    min_val = min(np.min(splits[\"y_test\"]), np.min(predictions[\"mean\"]))\n",
    "    max_val = max(np.max(splits[\"y_test\"]), np.max(predictions[\"mean\"]))\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], \"k--\", alpha=0.5)\n",
    "    \n",
    "    plt.xlabel(\"Actual\")\n",
    "    plt.ylabel(\"Predicted\")\n",
    "    plt.title(\"Predictions with 90% Credible Intervals\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = np.mean((splits[\"y_test\"] - predictions[\"mean\"]) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = np.mean(np.abs(splits[\"y_test\"] - predictions[\"mean\"]))\n",
    "    \n",
    "    print(f\"MSE: {mse:.6f}\")\n",
    "    print(f\"RMSE: {rmse:.6f}\")\n",
    "    print(f\"MAE: {mae:.6f}\")\n",
    "    \n",
    "    # Calculate coverage\n",
    "    coverage = np.mean((splits[\"y_test\"] >= predictions[\"lower\"]) & (splits[\"y_test\"] <= predictions[\"upper\"]))\n",
    "    print(f\"90% Credible Interval Coverage: {coverage:.2%}\")\n",
    "else:\n",
    "    print(\"Skipping predictions because NumPyro is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Sample from Posterior Predictive Distribution\n",
    "\n",
    "Let's sample from the posterior predictive distribution to get a sense of the uncertainty in our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "if NUMPYRO_AVAILABLE:\n",
    "    # Sample from posterior predictive distribution\n",
    "    n_samples = 100\n",
    "    samples = model.predict(normalized[\"X_test\"][:10], return_samples=True, n_samples=n_samples)\n",
    "    \n",
    "    # Plot samples\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    for i in range(10):  # Plot first 10 test points\n",
    "        plt.subplot(2, 5, i + 1)\n",
    "        \n",
    "        # Plot histogram of samples\n",
    "        plt.hist(samples[\"mu\"][:, i], bins=20, alpha=0.7, density=True)\n",
    "        \n",
    "        # Plot actual value\n",
    "        plt.axvline(splits[\"y_test\"][i], color=\"red\", linestyle=\"--\", label=\"Actual\")\n",
    "        \n",
    "        # Plot mean prediction\n",
    "        plt.axvline(np.mean(samples[\"mu\"][:, i]), color=\"blue\", linestyle=\"-\", label=\"Mean\")\n",
    "        \n",
    "        plt.title(f\"Test Point {i}\")\n",
    "        \n",
    "        if i == 0:\n",
    "            plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Skipping posterior sampling because NumPyro is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save and Load Model\n",
    "\n",
    "Let's save the model and then load it back to make sure it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "if NUMPYRO_AVAILABLE:\n",
    "    # Create models directory if it doesn't exist\n",
    "    os.makedirs(\"../models\", exist_ok=True)\n",
    "    \n",
    "    # Save model\n",
    "    model_path = \"../models/bayesian_regression.npz\"\n",
    "    model.save(model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "    \n",
    "    # Load model\n",
    "    loaded_model = BayesianRegression.load(model_path)\n",
    "    print(\"Model loaded successfully.\")\n",
    "    \n",
    "    # Make predictions with loaded model\n",
    "    loaded_predictions = loaded_model.predict_interval(normalized[\"X_test\"][:10], interval=0.9)\n",
    "    \n",
    "    # Compare predictions\n",
    "    print(\"\\nComparing predictions from original and loaded models:\")\n",
    "    comparison = pd.DataFrame({\n",
    "        \"Original Mean\": predictions[\"mean\"][:10],\n",
    "        \"Loaded Mean\": loaded_predictions[\"mean\"],\n",
    "        \"Original Lower\": predictions[\"lower\"][:10],\n",
    "        \"Loaded Lower\": loaded_predictions[\"lower\"],\n",
    "        \"Original Upper\": predictions[\"upper\"][:10],\n",
    "        \"Loaded Upper\": loaded_predictions[\"upper\"]\n",
    "    })\n",
    "    display(comparison)\n",
    "else:\n",
    "    print(\"Skipping model saving and loading because NumPyro is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "In this notebook, we have validated the Bayesian regression model for uncertainty quantification. We have:\n",
    "\n",
    "1. Loaded and prepared the data\n",
    "2. Trained a Bayesian regression model\n",
    "3. Examined the posterior distribution of the model parameters\n",
    "4. Made predictions with uncertainty estimates\n",
    "5. Sampled from the posterior predictive distribution\n",
    "6. Saved and loaded the model\n",
    "\n",
    "The Bayesian approach provides not only point predictions but also uncertainty estimates, which are crucial for risk management in financial applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
