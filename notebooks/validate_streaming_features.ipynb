{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming Features Validation\n",
    "\n",
    "This notebook validates the streaming features computed by the ETL job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Set up plotting\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize Spark session\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"validate_streaming_features\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run Streaming Feature Job in Test Mode\n",
    "\n",
    "First, let's run the streaming feature job in test mode to generate sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import the streaming feature module\n",
    "import sys\n",
    "import threading\n",
    "sys.path.append('../src/etl')\n",
    "\n",
    "import streaming_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Run the streaming job in a separate thread\n",
    "def run_streaming_job():\n",
    "    try:\n",
    "        streaming_features.main(source=\"delta\", sink=\"delta\", test_mode=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in streaming job: {e}\")\n",
    "\n",
    "# Start the streaming job\n",
    "streaming_thread = threading.Thread(target=run_streaming_job)\n",
    "streaming_thread.daemon = True  # This allows the thread to be killed when the notebook is closed\n",
    "streaming_thread.start()\n",
    "\n",
    "print(\"Streaming job started in test mode. It will run in the background.\")\n",
    "print(\"Let it run for a few minutes to generate data, then proceed with validation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Sample Tick Data\n",
    "\n",
    "Let's generate some sample tick data to test the streaming job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to generate sample tick data\n",
    "def generate_sample_ticks(symbols=['AAPL', 'MSFT', 'GOOGL'], num_ticks=100):\n",
    "    from datetime import datetime, timedelta\n",
    "    import random\n",
    "    \n",
    "    data = []\n",
    "    base_time = datetime.now()\n",
    "    \n",
    "    for symbol in symbols:\n",
    "        # Set base price for each symbol\n",
    "        if symbol == 'AAPL':\n",
    "            base_price = 150.0\n",
    "        elif symbol == 'MSFT':\n",
    "            base_price = 250.0\n",
    "        elif symbol == 'GOOGL':\n",
    "            base_price = 2000.0\n",
    "        else:\n",
    "            base_price = 100.0\n",
    "        \n",
    "        for i in range(num_ticks):\n",
    "            # Generate random price movement\n",
    "            price_change = (random.random() - 0.5) * 2.0  # Between -1 and 1\n",
    "            close_price = base_price + price_change\n",
    "            \n",
    "            # Generate OHLC data\n",
    "            high_price = close_price + random.random() * 0.5\n",
    "            low_price = close_price - random.random() * 0.5\n",
    "            open_price = low_price + random.random() * (high_price - low_price)\n",
    "            \n",
    "            # Generate volume\n",
    "            volume = int(random.random() * 10000) + 1000\n",
    "            \n",
    "            # Generate timestamp\n",
    "            timestamp = base_time + timedelta(seconds=i*10)\n",
    "            \n",
    "            # Create tick record\n",
    "            tick = {\n",
    "                'timestamp': timestamp.isoformat(),\n",
    "                'symbol': symbol,\n",
    "                'open': open_price,\n",
    "                'high': high_price,\n",
    "                'low': low_price,\n",
    "                'close': close_price,\n",
    "                'volume': volume\n",
    "            }\n",
    "            \n",
    "            data.append(tick)\n",
    "            \n",
    "            # Update base price for next tick\n",
    "            base_price = close_price\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df = df.sort_values('timestamp')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate sample tick data\n",
    "sample_ticks = generate_sample_ticks(num_ticks=50)\n",
    "\n",
    "# Display sample data\n",
    "print(\"Sample Tick Data:\")\n",
    "sample_ticks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save sample data to Parquet for the streaming job to process\n",
    "from datetime import datetime\n",
    "\n",
    "# Extract date for partitioning\n",
    "sample_ticks['date'] = sample_ticks['timestamp'].dt.date\n",
    "\n",
    "# Convert to Spark DataFrame\n",
    "spark_df = spark.createDataFrame(sample_ticks)\n",
    "\n",
    "# Save to Parquet\n",
    "output_dir = \"../data/raw/ticks/\" + datetime.now().strftime(\"%Y-%m-%d\")\n",
    "spark_df.write.mode(\"overwrite\").parquet(output_dir)\n",
    "\n",
    "print(f\"Saved sample tick data to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Validate Streaming Features\n",
    "\n",
    "Now, let's check if the streaming job is generating features from our sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to check for streaming features\n",
    "def check_streaming_features(max_attempts=10, delay_seconds=10):\n",
    "    for attempt in range(max_attempts):\n",
    "        try:\n",
    "            # Try to read the streaming features\n",
    "            streaming_df = spark.read.format(\"delta\").load(\"../data/features/streaming_test\")\n",
    "            \n",
    "            # If we got here, we found data\n",
    "            print(f\"Found streaming features on attempt {attempt + 1}\")\n",
    "            return streaming_df\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1}: No streaming features found yet. Waiting {delay_seconds} seconds...\")\n",
    "            time.sleep(delay_seconds)\n",
    "    \n",
    "    print(f\"No streaming features found after {max_attempts} attempts.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check for streaming features\n",
    "streaming_df = check_streaming_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# If we found streaming features, analyze them\n",
    "if streaming_df is not None:\n",
    "    # Show schema\n",
    "    print(\"Streaming Features Schema:\")\n",
    "    streaming_df.printSchema()\n",
    "    \n",
    "    # Show sample data\n",
    "    print(\"\\nSample Streaming Features:\")\n",
    "    streaming_df.show(5)\n",
    "    \n",
    "    # Convert to Pandas for easier analysis\n",
    "    streaming_pd = streaming_df.toPandas()\n",
    "    \n",
    "    # Display summary statistics\n",
    "    print(\"\\nStreaming Features Summary Statistics:\")\n",
    "    display(streaming_pd.describe())\n",
    "    \n",
    "    # Plot streaming features for a specific symbol\n",
    "    symbol = \"AAPL\"  # Change this to any symbol in your data\n",
    "    symbol_data = streaming_pd[streaming_pd['symbol'] == symbol].sort_values('timestamp')\n",
    "    \n",
    "    if len(symbol_data) > 0:\n",
    "        # Plot VWAP\n",
    "        plt.figure(figsize=(14, 7))\n",
    "        plt.plot(symbol_data['timestamp'], symbol_data['close'], label='Close Price')\n",
    "        plt.plot(symbol_data['timestamp'], symbol_data['vwap_1m'], label='1-min VWAP')\n",
    "        plt.plot(symbol_data['timestamp'], symbol_data['vwap_5m'], label='5-min VWAP')\n",
    "        plt.title(f'{symbol} Price and VWAP')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Price')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot Volatility\n",
    "        plt.figure(figsize=(14, 5))\n",
    "        plt.plot(symbol_data['timestamp'], symbol_data['volatility_1m'], label='1-min Volatility')\n",
    "        plt.plot(symbol_data['timestamp'], symbol_data['volatility_5m'], label='5-min Volatility')\n",
    "        plt.title(f'{symbol} Volatility')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Volatility')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot Momentum\n",
    "        plt.figure(figsize=(14, 5))\n",
    "        plt.plot(symbol_data['timestamp'], symbol_data['momentum_1m'], label='1-min Momentum')\n",
    "        plt.plot(symbol_data['timestamp'], symbol_data['momentum_5m'], label='5-min Momentum')\n",
    "        plt.title(f'{symbol} Momentum')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Momentum')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"No data available for symbol {symbol}\")\n",
    "else:\n",
    "    print(\"No streaming features found. Please check the streaming job.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Real-time Monitoring\n",
    "\n",
    "Let's set up a real-time monitor to watch for new streaming features as they arrive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to monitor streaming features in real-time\n",
    "def monitor_streaming_features(symbol=\"AAPL\", max_iterations=10, delay_seconds=5):\n",
    "    for iteration in range(max_iterations):\n",
    "        try:\n",
    "            # Read the latest streaming features\n",
    "            streaming_df = spark.read.format(\"delta\").load(\"../data/features/streaming_test\")\n",
    "            streaming_pd = streaming_df.toPandas()\n",
    "            \n",
    "            # Filter for the specified symbol\n",
    "            symbol_data = streaming_pd[streaming_pd['symbol'] == symbol].sort_values('timestamp')\n",
    "            \n",
    "            if len(symbol_data) > 0:\n",
    "                # Clear previous output\n",
    "                clear_output(wait=True)\n",
    "                \n",
    "                # Print iteration info\n",
    "                print(f\"Monitoring iteration {iteration + 1}/{max_iterations}\")\n",
    "                print(f\"Found {len(symbol_data)} records for {symbol}\")\n",
    "                print(f\"Latest timestamp: {symbol_data['timestamp'].max()}\")\n",
    "                \n",
    "                # Plot the latest data\n",
    "                plt.figure(figsize=(14, 12))\n",
    "                \n",
    "                # Price and VWAP\n",
    "                plt.subplot(3, 1, 1)\n",
    "                plt.plot(symbol_data['timestamp'], symbol_data['close'], label='Close Price')\n",
    "                plt.plot(symbol_data['timestamp'], symbol_data['vwap_1m'], label='1-min VWAP')\n",
    "                plt.title(f'{symbol} Price and VWAP')\n",
    "                plt.legend()\n",
    "                plt.grid(True)\n",
    "                \n",
    "                # Volatility\n",
    "                plt.subplot(3, 1, 2)\n",
    "                plt.plot(symbol_data['timestamp'], symbol_data['volatility_1m'], label='1-min Volatility')\n",
    "                plt.title(f'{symbol} Volatility')\n",
    "                plt.legend()\n",
    "                plt.grid(True)\n",
    "                \n",
    "                # Momentum\n",
    "                plt.subplot(3, 1, 3)\n",
    "                plt.plot(symbol_data['timestamp'], symbol_data['momentum_1m'], label='1-min Momentum')\n",
    "                plt.title(f'{symbol} Momentum')\n",
    "                plt.legend()\n",
    "                plt.grid(True)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "                # Display the latest few records\n",
    "                print(\"\\nLatest records:\")\n",
    "                display(symbol_data.tail(5))\n",
    "            else:\n",
    "                print(f\"No data available for symbol {symbol} in iteration {iteration + 1}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in iteration {iteration + 1}: {e}\")\n",
    "        \n",
    "        # Wait before next check\n",
    "        time.sleep(delay_seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Monitor streaming features\n",
    "monitor_streaming_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary and Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Features\n",
    "- VWAP (1-min, 5-min) provides volume-weighted price information\n",
    "- Volatility (1-min, 5-min) measures price variability\n",
    "- Momentum (1-min, 5-min) captures price trends\n",
    "\n",
    "### Real-time Processing\n",
    "- The streaming job successfully processes incoming tick data\n",
    "- Features are computed and stored in real-time\n",
    "- The system can handle continuous data streams\n",
    "\n",
    "### Next Steps\n",
    "- Integrate these streaming features into the Feast feature store\n",
    "- Use these features for real-time prediction and trading signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
